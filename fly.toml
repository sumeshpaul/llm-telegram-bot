# fly.toml app configuration file generated for llm-telegram-bot on 2025-05-18T08:42:48Z
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'llm-telegram-bot'
primary_region = 'ams'

[build]
  dockerfile = 'Dockerfile'

[http_service]
  internal_port = 8000
  force_https = true
  auto_stop_machines = 'stop'
  auto_start_machines = true
  min_machines_running = 0
  processes = ['app']

[vm]
  memory = '4096'
  cpu_kind = 'performance'
  cpus = 2

[mounts]
  source = "model_data"
  destination = "/mnt/models"
