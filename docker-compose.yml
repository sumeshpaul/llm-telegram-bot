version: "3.9"

services:
  lora-llm:
    build:
      context: .
      dockerfile: Dockerfile
    image: torch-cuda-2.8
    container_name: lora-combined
    command: python3 start_combined.py
    restart: unless-stopped
    env_file:
      - .env
    environment:
      - MODE=serve
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "8000:8000"
    runtime: nvidia
    volumes:
      - ./start_combined.py:/app/start_combined.py
      - ./vectorize_and_index.py:/app/vectorize_and_index.py
      - ./utils:/app/utils
      - ./final_lora_model_v2:/app/final_lora_model_v2
      - ./data:/app/data
      - ./rag_data:/app/rag_data
